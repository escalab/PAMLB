{"cells":[{"cell_type":"markdown","metadata":{"editable":false},"source":["# Building a Collaborative Filtering news recommender\n","In this notebook we pre-process and train a news recommender system based on the MIND dataset."]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","editable":false,"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from collections import Counter\n","from pytorch_lightning import seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import time\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor Cores are supported on NVIDIA GeForce RTX 3090\n"]}],"source":["hasTensorCores = False\n","# Check if CUDA (GPU) is available\n","if torch.cuda.is_available():\n","    # Get the current CUDA device\n","    current_device = torch.cuda.current_device()\n","\n","    # Get the device properties for the current device\n","    device_properties = torch.cuda.get_device_properties(current_device)\n","\n","    # Check if Tensor Cores are supported (compute capability >= 7.0)\n","    if device_properties.major >= 7:\n","        print(f'Tensor Cores are supported on {device_properties.name}')\n","        hasTensorCores = True\n","    else:\n","        print(f'Tensor Cores are not supported on {device_properties.name}')\n","else:\n","    print('CUDA (GPU) is not available. Tensor Cores require GPU support.')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Manual pre-processing of data\n","There exist a library to preprocess this data, but it's quite hard to use it. So instead, after a quick look on the data we have descided to create a own version of pre-processing and data creation for this lab. If you are interested in the full data take a look at it here: https://msnews.github.io/. For the first read, it is not necessary to understand what is being done here, and we will re-introduce the dataset in the modeling section, you can skip to **Output of data preprocessing**\n","\n","\n","### behaviors.tsv\n","\n","#### From documentation:  \n","The behaviors.tsv file contains the impression logs and users' news click histories. It has 5 columns divided by the tab symbol:\n","- Impression ID. The ID of an impression.  \n","- User ID. The anonymous ID of a user.  \n","- Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".  \n","- History. The news click history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.  \n","- Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.  "]},{"cell_type":"code","execution_count":24,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset originally consist of 156965 number of interactions.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>impressionId</th>\n","      <th>userId</th>\n","      <th>timestamp</th>\n","      <th>click_history</th>\n","      <th>impressions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>U13740</td>\n","      <td>11/11/2019 9:05:58 AM</td>\n","      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n","      <td>N55689-1 N35729-0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>U91836</td>\n","      <td>11/12/2019 6:11:30 PM</td>\n","      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n","      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>U73700</td>\n","      <td>11/14/2019 7:01:48 AM</td>\n","      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n","      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>U34670</td>\n","      <td>11/11/2019 5:28:05 AM</td>\n","      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n","      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>U8125</td>\n","      <td>11/12/2019 4:11:21 PM</td>\n","      <td>N10078 N56514 N14904 N33740</td>\n","      <td>N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   impressionId  userId              timestamp  \\\n","0             1  U13740  11/11/2019 9:05:58 AM   \n","1             2  U91836  11/12/2019 6:11:30 PM   \n","2             3  U73700  11/14/2019 7:01:48 AM   \n","3             4  U34670  11/11/2019 5:28:05 AM   \n","4             5   U8125  11/12/2019 4:11:21 PM   \n","\n","                                       click_history  \\\n","0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n","1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n","2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n","3  N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n","4                        N10078 N56514 N14904 N33740   \n","\n","                                         impressions  \n","0                                  N55689-1 N35729-0  \n","1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n","2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n","3                N35729-0 N33632-0 N49685-1 N27581-0  \n","4  N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...  "]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["raw_behaviour = pd.read_csv(\n","    \"<PATH-TO-RECOMMENDER>/mind-news-dataset/MINDsmall_train/behaviors.tsv\", \n","    sep=\"\\t\",\n","    names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n","\n","print(f\"The dataset originally consist of {len(raw_behaviour)} number of interactions.\")\n","raw_behaviour.head()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["single_user=\"U13740\"\n","single_user_behaviour=raw_behaviour[(raw_behaviour.userId == single_user)]\n","print(len(single_user_behaviour))\n","print(f\"single user data size estimation {single_user_behaviour.memory_usage(index=True, deep=True).sum()} bytes\")"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Load article data\n","We also need to get the content information of each article. We will use the news.tsv file to index the items."]},{"cell_type":"code","execution_count":26,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The article data consist in total of 51282 number of articles.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>itemId</th>\n","      <th>category</th>\n","      <th>subcategory</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>url</th>\n","      <th>title_entities</th>\n","      <th>abstract_entities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>N55528</td>\n","      <td>lifestyle</td>\n","      <td>lifestyleroyals</td>\n","      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n","      <td>Shop the notebooks, jackets, and more that the...</td>\n","      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n","      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>N19639</td>\n","      <td>health</td>\n","      <td>weightloss</td>\n","      <td>50 Worst Habits For Belly Fat</td>\n","      <td>These seemingly harmless habits are holding yo...</td>\n","      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n","      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n","      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>N61837</td>\n","      <td>news</td>\n","      <td>newsworld</td>\n","      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n","      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n","      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n","      <td>[]</td>\n","      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>N53526</td>\n","      <td>health</td>\n","      <td>voices</td>\n","      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n","      <td>I felt like I was a fraud, and being an NBA wi...</td>\n","      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n","      <td>[]</td>\n","      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>N38324</td>\n","      <td>health</td>\n","      <td>medical</td>\n","      <td>How to Get Rid of Skin Tags, According to a De...</td>\n","      <td>They seem harmless, but there's a very good re...</td>\n","      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n","      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n","      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   itemId   category      subcategory  \\\n","0  N55528  lifestyle  lifestyleroyals   \n","1  N19639     health       weightloss   \n","2  N61837       news        newsworld   \n","3  N53526     health           voices   \n","4  N38324     health          medical   \n","\n","                                               title  \\\n","0  The Brands Queen Elizabeth, Prince Charles, an...   \n","1                      50 Worst Habits For Belly Fat   \n","2  The Cost of Trump's Aid Freeze in the Trenches...   \n","3  I Was An NBA Wife. Here's How It Affected My M...   \n","4  How to Get Rid of Skin Tags, According to a De...   \n","\n","                                            abstract  \\\n","0  Shop the notebooks, jackets, and more that the...   \n","1  These seemingly harmless habits are holding yo...   \n","2  Lt. Ivan Molchanets peeked over a parapet of s...   \n","3  I felt like I was a fraud, and being an NBA wi...   \n","4  They seem harmless, but there's a very good re...   \n","\n","                                             url  \\\n","0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n","1  https://assets.msn.com/labs/mind/AAB19MK.html   \n","2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n","3  https://assets.msn.com/labs/mind/AACk2N6.html   \n","4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n","\n","                                      title_entities  \\\n","0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n","1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n","2                                                 []   \n","3                                                 []   \n","4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n","\n","                                   abstract_entities  \n","0                                                 []  \n","1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n","2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n","3  [{\"Label\": \"National Basketball Association\", ...  \n","4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  "]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["news = pd.read_csv(\n","    \"<PATH-TO-RECOMMENDER>/mind-news-dataset/MINDsmall_train/news.tsv\", \n","    sep=\"\\t\",\n","    names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n","print(f\"The article data consist in total of {len(news)} number of articles.\")\n","news.head()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["Now we need to process the click history and impressions. We first need to decode impressions into clicks and non-clicks."]},{"cell_type":"code","execution_count":27,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 1.55 s, sys: 46.7 ms, total: 1.6 s\n","Wall time: 1.6 s\n"]}],"source":["%%time\n","# Function to split the impressions and clicks into two seperate lists\n","def process_impression(impression_list):\n","    list_of_strings = impression_list.split()\n","    click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '1']\n","    non_click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '0']\n","    return click,non_click\n","\n","# We can then indexize these two new columns:\n","raw_behaviour['click'], raw_behaviour['noclicks'] = zip(*raw_behaviour['impressions'].map(process_impression))\n","\n","# Convert timestamp value to hours since epoch\n","raw_behaviour['epochhrs'] = pd.to_datetime(raw_behaviour['timestamp']).values.astype(np.int64)/(1e6)/1000/3600\n","raw_behaviour['epochhrs'] = raw_behaviour['epochhrs'].round()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 803 µs, sys: 70 µs, total: 873 µs\n","Wall time: 911 µs\n"]}],"source":["%%time\n","# pre-process for a single user\n","def process_impression(impression_list):\n","    list_of_strings = impression_list.split()\n","    click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '1']\n","    non_click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '0']\n","    return click,non_click\n","# We can then indexize these two new columns:\n","single_user_behaviour['click'], single_user_behaviour['noclicks'] = zip(*single_user_behaviour['impressions'].map(process_impression))\n","\n","# Convert timestamp value to hours since epoch\n","single_user_behaviour['epochhrs'] = pd.to_datetime(single_user_behaviour['timestamp']).values.astype(np.int64)/(1e6)/1000/3600\n","single_user_behaviour['epochhrs'] = single_user_behaviour['epochhrs'].round()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# print(f\"processed single user data size estimation {single_user_behaviour.memory_usage(index=True, deep=True).sum()} bytes\")"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Click History\n","\n","In the dataset we can see that a large number of items and users does not have sufficent amount of clicks. This is since we are working with a smaller version of the  MIND dataset that contains 50k users instead of the full version of 1 million users. \n","Therefore it will be hard to learn the user and item embeddings by only relying on the interactions e.g. the `click` and `noclicks`.\n","\n","To resolve this issue in the lab, we will expand the click_history column, which will add about 7 times more interactions than the original data. However, note that these events don't have any information about which articles were shown to the user e.g. the impressions or noclicks.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset after pre-processing consist of 1162402 number of interactions.\n","CPU times: user 1.22 s, sys: 67.4 ms, total: 1.29 s\n","Wall time: 1.28 s\n"]}],"source":["%%time\n","# If there exists several clicks in one session, expand to new observation\n","raw_behaviour = raw_behaviour.explode(\"click\").reset_index(drop=True)\n","\n","# Extract the clicks from the previous clicks\n","click_history = raw_behaviour[[\"userId\",\"click_history\"]].drop_duplicates().dropna()\n","click_history[\"click_history\"] = click_history.click_history.map(lambda x: x.split())\n","click_history = click_history.explode(\"click_history\").rename(columns={\"click_history\":\"click\"})\n","# Dummy time set to earlies epochhrs in raw_behaviour as we don't know when these events took place.\n","click_history[\"epochhrs\"] = raw_behaviour.epochhrs.min() \n","click_history[\"noclicks\"] = pd.Series([[] for _ in range(len(click_history.index))])\n","\n","# concatenate historical clicks with the raw_behaviour\n","raw_behaviour = pd.concat([raw_behaviour,click_history],axis=0).reset_index(drop=True)\n","print(f\"The dataset after pre-processing consist of {len(raw_behaviour)} number of interactions.\")"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Cold start problem\n","\n","Still after doing our pre-processing and adding the `click_history` to the `click` column, we can see that a large number of items does not have sufficent amount of clicks.  This can be thought of as a [cold start problem](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)).\n","To adjust for this we will remove items from the `raw_behaviour` that falls under the `min_click_cutoff`. \n"," "]},{"cell_type":"code","execution_count":31,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of items that have less than 100 clicks make up 93.852 % of the total, and these will be removed.\n"]}],"source":["min_click_cutoff = 100\n","print(f'Number of items that have less than {min_click_cutoff} clicks make up',np.round(np.mean(raw_behaviour.groupby(\"click\").size() < min_click_cutoff)*100,3),'% of the total, and these will be removed.') "]},{"cell_type":"code","execution_count":32,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 873 ms, sys: 0 ns, total: 873 ms\n","Wall time: 872 ms\n"]}],"source":["%%time\n","# remove items with less clicks than min_click_cutoff\n","raw_behaviour = raw_behaviour[raw_behaviour.groupby(\"click\")[\"userId\"].transform('size') >= min_click_cutoff].reset_index(drop=True)\n","# Get a set with all the unique items\n","click_set = set(raw_behaviour['click'].unique())\n","\n","# remove items for impressions that is not avaiable in the click set (the items that we will be training on)\n","raw_behaviour['noclicks'] = raw_behaviour['noclicks'].apply(lambda impressions: [impression for impression in impressions if impression in click_set])"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Output of data preprocessing\n","In this preprocessing we have processed behaviour data, article data and user data. The main component is `behaviour`, and for collaborative filtering purposes this is all we need. However, if we want to utilize content data on the news items some additional preprocessing on the `news` dataframe must be applied."]},{"cell_type":"code","execution_count":33,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of interactions in the behaviour dataset: 781871\n","Number of users in the behaviour dataset: 49832\n","Number of articles in the behaviour dataset: 2451\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epochhrs</th>\n","      <th>userId</th>\n","      <th>click</th>\n","      <th>noclicks</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>437073.0</td>\n","      <td>U13740</td>\n","      <td>N55689</td>\n","      <td>[N35729]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>437106.0</td>\n","      <td>U91836</td>\n","      <td>N17059</td>\n","      <td>[N20678, N39317, N58114, N20495, N42977, N1459...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>437143.0</td>\n","      <td>U73700</td>\n","      <td>N23814</td>\n","      <td>[N23877, N35389, N49712, N16844, N59685, N2344...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>437069.0</td>\n","      <td>U34670</td>\n","      <td>N49685</td>\n","      <td>[N35729, N33632, N27581]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>437083.0</td>\n","      <td>U19739</td>\n","      <td>N33619</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epochhrs  userId   click                                           noclicks\n","0  437073.0  U13740  N55689                                           [N35729]\n","1  437106.0  U91836  N17059  [N20678, N39317, N58114, N20495, N42977, N1459...\n","2  437143.0  U73700  N23814  [N23877, N35389, N49712, N16844, N59685, N2344...\n","3  437069.0  U34670  N49685                           [N35729, N33632, N27581]\n","4  437083.0  U19739  N33619                                                 []"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["## Select the columns that we now want to use for further analysis\n","behaviour = raw_behaviour[['epochhrs','userId','click','noclicks']].copy()\n","\n","print('Number of interactions in the behaviour dataset:', behaviour.shape[0])\n","print('Number of users in the behaviour dataset:', behaviour.userId.nunique())\n","print('Number of articles in the behaviour dataset:', behaviour.click.nunique())\n","\n","behaviour.head()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Train / Test Split + indexing\n","\n","Before we carry on to define our first model we first need to apply indexizing for the users and items in the behaviour dataframe,\n","as pytorch requires integer indicies instead of strings for user and item IDs. \n","\n","We do this by two dictionaries:\n","\n","- `ind2item`: mapping the item indicies given in behaviour to the real item Id given in the dataset.\n","- `ind2user`: mapping the user indicies given in behaviour to the real user Id given in the dataset.\n","\n","Note that we also create `item2ind` and `user2ind` to do the reverse.\n","\n","The indexing will be created based on the training data, where new unseen articles in the validation set will get the index 0.\n","We will use 90% for training 10% for validation, when we split the data it's important to make use of temporal `epochhrs` to divide the data, as a regular random split in this case does not make sense in recommender systems.\n"]},{"cell_type":"code","execution_count":34,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Indexing time 0.1737 sec\n"]}],"source":["# Let us use the last 10pct of the data as our validation data:\n","test_time_th = behaviour['epochhrs'].quantile(0.9)\n","train = behaviour[behaviour['epochhrs']< test_time_th].copy()\n","\n","indexing_time = 0.0\n","start_time = time.time()\n","## Indexize items\n","# Allocate a unique index for each item, but let the zeroth index be a UNK index:\n","ind2item = {idx +1: itemid for idx, itemid in enumerate(train.click.unique())}\n","item2ind = {itemid : idx for idx, itemid in ind2item.items()}\n","indexing_time += time.time() - start_time\n","\n","train['noclicks'] = train['noclicks'].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n","train['click'] = train['click'].map(lambda item: item2ind.get(item, 0))\n","\n","start_time = time.time()\n","## Indexize users\n","# Allocate a unique index for each user, but let the zeroth index be a UNK index:\n","ind2user = {idx +1: userid for idx, userid in enumerate(train['userId'].unique())}\n","user2ind = {userid : idx for idx, userid in ind2user.items()}\n","indexing_time += time.time() - start_time\n","\n","# Create a new column with userIdx:\n","train['userIdx'] = train['userId'].map(lambda x: user2ind.get(x,0))\n","\n","# Repeat for validation\n","valid =  behaviour[behaviour['epochhrs']>= test_time_th].copy()\n","start_time = time.time()\n","valid[\"click\"] = valid[\"click\"].map(lambda item: item2ind.get(item, 0))\n","valid[\"noclicks\"] = valid[\"noclicks\"].map(lambda list_of_items: [item2ind.get(l, 0) for l in list_of_items])\n","valid[\"userIdx\"] = valid[\"userId\"].map(lambda x: user2ind.get(x,0))\n","indexing_time += time.time() - start_time\n","print(f\"Indexing time {indexing_time:.4f} sec\")"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# Modeling & Negative sampling\n","We want to make a matrix factorization model where each user $u$ has a d-dimensional parameter vector $z_u$ and each item $i$ has a parameter vector $v_i$.\n","\n","Second, to simplify the computation of things and as we do not have a `noclicks` for every `click` interaction we will only utilize two **known** things in the training phase: The item the `userIdx` and `click`. However, as we want to model the binary behavior in terms of clicks and non-clicks we will make use of something called negative sampling. With negative sampling - we will draw a sample a random negative item for each known user-click combination to express  the lack of preference by the user for the sampled item."]},{"cell_type":"code","execution_count":35,"metadata":{"editable":false,"trusted":true},"outputs":[],"source":["class MindDataset(Dataset):\n","    # A fairly simple torch dataset module that can take a pandas dataframe (as above), \n","    # and convert the relevant fields into a dictionary of arrays that can be used in a dataloader\n","    def __init__(self, df):\n","        # Create a dictionary of tensors out of the dataframe\n","        self.data = {\n","            'userIdx' : torch.tensor(df.userIdx.values.astype(np.int64)),\n","            'click' : torch.tensor(df.click.values.astype(np.int64))\n","        }\n","    def __len__(self):\n","        return len(self.data['userIdx'])\n","    def __getitem__(self, idx):\n","        return {key: val[idx] for key, val in self.data.items()}"]},{"cell_type":"code","execution_count":36,"metadata":{"editable":false,"trusted":true},"outputs":[],"source":["# Build datasets and dataloaders of train and validation dataframes:\n","bs = 1024\n","bs_val = 1\n","ds_train = MindDataset(train)\n","train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True)\n","ds_valid = MindDataset(valid)\n","valid_loader = DataLoader(ds_valid, batch_size=bs_val, shuffle=False)\n","\n","batch = next(iter(train_loader))"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["## Model\n","\n","#### Framework\n","We will use pytorch-lightning to define and train our model. It is a high-level framework (similar to fastAI) but with a slightly different way of defining things. It is my personal go-to framework and is very flexible. For more information, see https://pytorch-lightning.readthedocs.io/.\n","\n","#### The model\n","We assume that each interaction goes as follow: the user is presented with two items: the click and no-click item, where the no-click item will be randomly chosen with negative sampling. After the user reviewed both items, she will choose the most relevant one. This can be modeled as a categorical distirbution with two options (yes, you could do binomial). There is a loss function in pytorch for this already, called the `F.binary_cross_entropy` that we will use."]},{"cell_type":"code","execution_count":37,"metadata":{"editable":false,"trusted":true},"outputs":[],"source":["# Build a matrix factorization model\n","class NewsMF(pl.LightningModule):\n","    def __init__(self, num_users, num_items, dim = 10):\n","        super().__init__()\n","        self.dim=dim\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        \n","        if num_users is not None and num_items is not None:\n","            self.useremb = nn.Embedding(num_embeddings=num_users, embedding_dim=dim)\n","            self.itememb = nn.Embedding(num_embeddings=num_items, embedding_dim=dim)\n","\n","        # self.useremb = nn.Embedding(num_embeddings=num_users, embedding_dim=dim)\n","        # self.itememb = nn.Embedding(num_embeddings=num_items, embedding_dim=dim)\n","\n","    def forward(self, userIdx, click):\n","        uservec = self.useremb(userIdx)\n","        itemvec_click = self.itememb(click)\n","\n","        # Compute scores for the positive interactions\n","        scores = torch.sigmoid((uservec * itemvec_click).sum(-1).unsqueeze(-1))\n","        \n","        return scores\n","\n","    def step(self, batch, batch_idx, phase=\"train\"):\n","        batch_size = batch['userIdx'].size(0)\n","        uservec = self.useremb(batch['userIdx'])       \n","        itemvec_click = self.itememb(batch['click'])\n","        \n","        # For each positive interaction,sample a random negative\n","        neg_sample = torch.randint_like(batch[\"click\"],1,self.num_items)\n","        itemvec_noclick = self.itememb(neg_sample)\n","        \n","        score_click = torch.sigmoid((uservec*itemvec_click).sum(-1).unsqueeze(-1))\n","        score_noclick =  torch.sigmoid((uservec*itemvec_noclick).sum(-1).unsqueeze(-1))\n","\n","        # Compute loss as binary cross entropy (categorical distribution between the clicked and the no clicked item)\n","        scores_all = torch.concat((score_click, score_noclick), dim=1)\n","        target_all = torch.concat((torch.ones_like(score_click), torch.zeros_like(score_noclick)),dim=1)\n","        loss = F.binary_cross_entropy(scores_all, target_all)\n","        return loss\n","    \n","    \n","    def training_step(self, batch, batch_idx):\n","        return self.step(batch, batch_idx, \"train\")\n","    \n","    def validation_step(self, batch, batch_idx):\n","        # for now, just do the same computation as during training\n","        return self.step(batch, batch_idx, \"val\")\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## NewsMF Model Training"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Seed set to 42\n","/nfshome/yhu130/miniconda3/envs/rec/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /nfshome/yhu130/miniconda3/envs/rec/lib/python3.8/si ...\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type      | Params\n","--------------------------------------\n","0 | useremb | Embedding | 2.5 M \n","1 | itememb | Embedding | 113 K \n","--------------------------------------\n","2.6 M     Trainable params\n","0         Non-trainable params\n","2.6 M     Total params\n","10.335    Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Set float32 matrix multiplication precision to medium\n","                                                                             "]},{"name":"stderr","output_type":"stream","text":["/nfshome/yhu130/miniconda3/envs/rec/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n","/nfshome/yhu130/miniconda3/envs/rec/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 686/686 [01:06<00:00, 10.36it/s, v_num=11] "]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=2` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 686/686 [01:06<00:00, 10.32it/s, v_num=11]\n","CPU times: user 2min 13s, sys: 10.6 s, total: 2min 24s\n","Wall time: 2min 13s\n"]}],"source":["%%time\n","seed_everything(42, workers=True)\n","# Model initialization\n","model = NewsMF(num_users=len(ind2user) + 1, num_items = len(ind2item) + 1, dim = 50)\n","\n","if hasTensorCores:\n","    # float32 matrix multiplication precision (choose 'medium' or 'high')\n","    # trading precision with performance\n","    mode = 'medium'\n","    torch.set_float32_matmul_precision(mode)\n","    print(f\"Set float32 matrix multiplication precision to {mode}\")\n","\n","# Create a trainer\n","trainer = pl.Trainer(\n","    max_epochs=2, \n","    accelerator=\"gpu\",\n","    deterministic = True,\n","    enable_checkpointing=True,\n",")\n","\n","# Training loop\n","trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n","\n","# Save the model\n","checkpoint_path = '<PATH-TO-RECOMMENDER>/MIND_model_ckpt/myMFmodel.ckpt'\n","trainer.save_checkpoint(checkpoint_path)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 151 ms, sys: 3.46 ms, total: 154 ms\n","Wall time: 11 ms\n"]},{"data":{"text/plain":["NewsMF(\n","  (useremb): Embedding(49397, 50)\n","  (itememb): Embedding(2279, 50)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","# Load pre-trained model from ckpt for predictions\n","pretrained_model = NewsMF(num_users=len(ind2user) + 1, num_items=len(ind2item) + 1, dim=50)\n","checkpoint_path = '<PATH-TO-RECOMMENDER>/MIND_model_ckpt/myMFmodel.ckpt'\n","\n","checkpoint = torch.load(checkpoint_path)\n","pretrained_model.load_state_dict(checkpoint, strict=False)\n","\n","pretrained_model.eval()\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2.16 s, sys: 2.17 ms, total: 2.17 s\n","Wall time: 2.17 s\n"]}],"source":["%%time\n","all_predictions = []\n","for batch in valid_loader:\n","    userIdx = batch['userIdx']\n","    click = batch['click']\n","\n","    with torch.no_grad():\n","        prediction = pretrained_model(userIdx, click)\n","    all_predictions.append(prediction)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Avg single inference time 0.012652ms\n","Avg post-processing time 0.000462ms\n","       itemId       category              subcategory  \\\n","33871  N18887         sports     football_ncaa_videos   \n","39270  N52622         sports             football_nfl   \n","43062  N35172  entertainment  entertainment-celebrity   \n","\n","                                                   title  \\\n","33871            Is the Alabama Football Dynasty Ending?   \n","39270  The NFL Hot Seat: Sean McVay ran out of magic,...   \n","43062  The Giudice family reunites and more ICYMI cel...   \n","\n","                                                abstract  \\\n","33871  Is the Alabama football dynasty ending? Max Br...   \n","39270                                                NaN   \n","43062  Giudice family reunites in Italy, Keanu Reeves...   \n","\n","                                                 url  \\\n","33871  https://assets.msn.com/labs/mind/BBWECWK.html   \n","39270  https://assets.msn.com/labs/mind/BBWB9g6.html   \n","43062  https://assets.msn.com/labs/mind/BBWtrzi.html   \n","\n","                                          title_entities  \\\n","33871  [{\"Label\": \"Alabama Crimson Tide football\", \"T...   \n","39270  [{\"Label\": \"National Football League\", \"Type\":...   \n","43062  [{\"Label\": \"Teresa Giudice\", \"Type\": \"P\", \"Wik...   \n","\n","                                       abstract_entities  \n","33871  [{\"Label\": \"Alabama Crimson Tide football\", \"T...  \n","39270                                                 []  \n","43062  [{\"Label\": \"Teresa Giudice\", \"Type\": \"P\", \"Wik...  \n"]}],"source":["# Avg single inference time\n","time_elapsed = 0.0\n","single_predictions = []\n","prediction_times = []\n","# u_, c_ = torch.Tensor(), torch.Tensor() \n","for batch in valid_loader: # bs_valid = 1\n","    userIdx = batch['userIdx']\n","    click = batch['click']\n","\n","    with torch.no_grad():\n","        start_time = time.time()\n","        prediction = pretrained_model(userIdx, click)\n","        prediction_times.append(time.time() - start_time)\n","        single_predictions.append(prediction)\n","       \n","print(f\"Avg single inference time {(sum(prediction_times)/len(prediction_times))*1000:.6f}ms\")\n","\n","start_time = time.time()\n","single_predictions = torch.cat(single_predictions, dim=0)\n","top_k_indices = torch.topk(single_predictions, k=3, dim=0).indices\n","# Filter for top k suggested items\n","filters = [ind2item[ix.item()] for ix in top_k_indices]\n","results = news[news[\"itemId\"].isin(filters)]\n","print(f\"Avg post-processing time {(time.time() - start_time)/len(prediction_times)*1000:.6f}ms\")\n","print(results)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["       itemId       category              subcategory  \\\n","33871  N18887         sports     football_ncaa_videos   \n","39270  N52622         sports             football_nfl   \n","43062  N35172  entertainment  entertainment-celebrity   \n","\n","                                                   title  \\\n","33871            Is the Alabama Football Dynasty Ending?   \n","39270  The NFL Hot Seat: Sean McVay ran out of magic,...   \n","43062  The Giudice family reunites and more ICYMI cel...   \n","\n","                                                abstract  \\\n","33871  Is the Alabama football dynasty ending? Max Br...   \n","39270                                                NaN   \n","43062  Giudice family reunites in Italy, Keanu Reeves...   \n","\n","                                                 url  \\\n","33871  https://assets.msn.com/labs/mind/BBWECWK.html   \n","39270  https://assets.msn.com/labs/mind/BBWB9g6.html   \n","43062  https://assets.msn.com/labs/mind/BBWtrzi.html   \n","\n","                                          title_entities  \\\n","33871  [{\"Label\": \"Alabama Crimson Tide football\", \"T...   \n","39270  [{\"Label\": \"National Football League\", \"Type\":...   \n","43062  [{\"Label\": \"Teresa Giudice\", \"Type\": \"P\", \"Wik...   \n","\n","                                       abstract_entities  \n","33871  [{\"Label\": \"Alabama Crimson Tide football\", \"T...  \n","39270                                                 []  \n","43062  [{\"Label\": \"Teresa Giudice\", \"Type\": \"P\", \"Wik...  \n","CPU times: user 37.7 ms, sys: 104 µs, total: 37.8 ms\n","Wall time: 37.1 ms\n"]}],"source":["%%time\n","# Concatenate the predictions from all batches along the batch dimension\n","all_predictions = torch.cat(all_predictions, dim=0)\n","top_k = 3\n","top_k_indices = torch.topk(all_predictions, k=top_k, dim=0).indices\n","\n","# Filter for top k suggested items\n","filters = [ind2item[ix.item()] for ix in top_k_indices]\n","print(news[news[\"itemId\"].isin(filters)])"]}],"metadata":{"kernelspec":{"display_name":"recommender","language":"python","name":"rec"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
